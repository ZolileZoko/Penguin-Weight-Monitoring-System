{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "49458f59-0c56-42b7-89e6-b39014a41a6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100k-entry dummy dataset saved as 'dummy_penguin_weights_100k.csv'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Number of entries\n",
    "num_entries = 100_000\n",
    "\n",
    "# Possible colony locations\n",
    "locations = [\"Boulders Beach\", \"Stony Point\", \"Dassen Island\", \"Robben Island\", \"Bird Island\"]\n",
    "\n",
    "# Generate random timestamps within a 2-year range\n",
    "start_time = datetime(2022, 1, 1)\n",
    "timestamps = [start_time + timedelta(minutes=random.randint(0, 2 * 365 * 24 * 60)) for _ in range(num_entries)]\n",
    "\n",
    "# Generate RFID tags (10-digit numbers)\n",
    "rfids = [f'{random.randint(1000000000, 9999999999)}' for _ in range(num_entries)]\n",
    "\n",
    "# Weights (in grams), normally distributed around 3000g Â± 300g\n",
    "weights = np.random.normal(loc=3000, scale=300, size=num_entries).clip(1800, 4500)\n",
    "\n",
    "# Sex (some may be unknown)\n",
    "sexes = np.random.choice(['M', 'F', 'Unknown'], size=num_entries, p=[0.45, 0.45, 0.1])\n",
    "\n",
    "# Age category\n",
    "age_categories = np.random.choice(['Chick', 'Juvenile', 'Adult'], size=num_entries, p=[0.2, 0.3, 0.5])\n",
    "\n",
    "# Measurement validity (simulate some invalid entries)\n",
    "valid_flags = np.random.choice([True, False], size=num_entries, p=[0.95, 0.05])\n",
    "\n",
    "# Locations\n",
    "colony_locations = np.random.choice(locations, size=num_entries)\n",
    "\n",
    "# Construct DataFrame\n",
    "df = pd.DataFrame({\n",
    "    'timestamp': timestamps,\n",
    "    'location': colony_locations,\n",
    "    'rfid': rfids,\n",
    "    'weight_g': weights.astype(int),\n",
    "    'sex': sexes,\n",
    "    'age_category': age_categories,\n",
    "    'valid_measurement': valid_flags\n",
    "})\n",
    "\n",
    "# Save to CSV\n",
    "df.to_csv(\"dummy_penguin_weights_100k.csv\", index=False)\n",
    "print(\"100k-entry dummy dataset saved as 'dummy_penguin_weights_100k.csv'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "21ad68e3-57c2-4290-bc23-0696787baf72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               timestamp       location        rfid  weight_g\n",
      "55   2022-08-01 16:24:00    Bird Island  4211421873      2141\n",
      "252  2022-12-06 14:07:00    Stony Point  5641961670      3805\n",
      "301  2022-12-27 13:55:00    Stony Point  3434443855      3807\n",
      "583  2023-09-01 04:45:00    Bird Island  5440534126      1982\n",
      "592  2023-03-03 21:49:00  Robben Island  4662180488      3828\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# 1. Load and filter the data\n",
    "df = pd.read_csv(\"dummy_penguin_weights_100k.csv\")\n",
    "df = df[df['valid_measurement'] == True]  # Use only valid data\n",
    "\n",
    "# 2. Select features\n",
    "X = df[['weight_g']]  # you can add more features later\n",
    "\n",
    "# 3. Normalize the data (optional but helps)\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# 4. Train Isolation Forest\n",
    "model = IsolationForest(n_estimators=100, contamination=0.01, random_state=42)\n",
    "model.fit(X_scaled)\n",
    "\n",
    "# 5. Predict anomalies\n",
    "df['anomaly'] = model.predict(X_scaled)  # -1 = anomaly, 1 = normal\n",
    "anomalies = df[df['anomaly'] == -1]\n",
    "\n",
    "# 6. Show some anomalies\n",
    "print(anomalies[['timestamp', 'location', 'rfid', 'weight_g']].head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "12aa007b-8787-47b2-8ba7-7240972b4d06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              timestamp        location        rfid  weight_g\n",
      "128 2022-03-06 08:31:00     Bird Island  9090307981      2593\n",
      "213 2022-10-31 23:56:00   Robben Island  3134069470      3087\n",
      "338 2022-03-15 01:52:00     Stony Point  3559797170      2508\n",
      "465 2023-07-30 21:08:00     Stony Point  6126293435      3301\n",
      "723 2022-05-30 22:06:00  Boulders Beach  1312332384      2785\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.ensemble import IsolationForest\n",
    "\n",
    "# Assume df is your DataFrame already loaded with columns: timestamp, location, rfid, weight_g, sex, age_category\n",
    "\n",
    "# 1. Extract time features\n",
    "df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "df['hour_of_day'] = df['timestamp'].dt.hour\n",
    "df['day_of_week'] = df['timestamp'].dt.dayofweek\n",
    "\n",
    "# 2. Define categorical and numerical columns\n",
    "categorical_cols = ['location', 'sex', 'age_category']\n",
    "numerical_cols = ['weight_g', 'hour_of_day', 'day_of_week']\n",
    "\n",
    "# 3. Create preprocessing pipeline: one-hot encode categorical, scale numerical\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_cols),\n",
    "        ('num', StandardScaler(), numerical_cols)\n",
    "    ])\n",
    "\n",
    "# 4. Fit and transform the data\n",
    "X = preprocessor.fit_transform(df)\n",
    "\n",
    "# Note: X is a sparse matrix if OneHotEncoder returns sparse; convert to dense if needed\n",
    "if hasattr(X, \"toarray\"):\n",
    "    X = X.toarray()\n",
    "\n",
    "# 5. Train Isolation Forest model\n",
    "model = IsolationForest(n_estimators=100, contamination=0.01, random_state=42)\n",
    "model.fit(X)\n",
    "\n",
    "# 6. Predict anomalies (-1 = anomaly, 1 = normal)\n",
    "df['anomaly'] = model.predict(X)\n",
    "\n",
    "# 7. Extract anomalies\n",
    "anomalies = df[df['anomaly'] == -1]\n",
    "\n",
    "# 8. Show some anomalies\n",
    "print(anomalies[['timestamp', 'location', 'rfid', 'weight_g']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee85f601-da18-43e2-aca8-3c323ffa8516",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
